# AYO.CO.ID VENUE SCRAPING PROJECT - KNOWLEDGE BASE

## PROJECT OVERVIEW
This project scrapes sport facility booking information from ayo.co.id, a venue booking website. The main challenge was handling JavaScript-rendered content that loads dynamically after the initial page load.

## TARGET WEBSITE STRUCTURE

### 1. VENUE LIST PAGE
- **URL Pattern**: `/venues?sortby=5&tipe=venue&lokasi=Kota+Jakarta+Selatan&cabor=7`
- **Pagination**: `/venues?page=X` where X is the page number
- **Venue Cards**: Located in `div.venue-card-item` elements
- **Venue Name**: Extracted from `img` tag's `alt` attribute within `div > a > div > img`
- **Venue URL**: Extracted from `a` tag's `href` attribute
- **Pagination Info**: Found in `div.venue-pagination > ul.pagination > li > a` with `rel="next"`

### 2. VENUE DETAIL PAGE
- **URL Pattern**: `/v/{venue-slug}` (e.g., `/v/ambassador-tennis`)
- **Field Container**: Empty `div#field-list-container` in initial HTML
- **Dynamic Content**: Field information loaded via JavaScript after page load
- **JavaScript Template**: `desktopFieldContainer` template generates field HTML

## KEY CHALLENGES IDENTIFIED

### 1. URL ENCODING ISSUE
- **Problem**: `urllib.parse.urlencode()` converts `+` to `%2B`
- **Solution**: Manual query string construction using `"&".join(params)`
- **Reason**: Website expects `+` characters, not URL-encoded `%2B`

### 2. DYNAMIC CONTENT LOADING
- **Problem**: Field and slot information loaded via JavaScript after page load
- **Initial HTML**: Contains empty `field-list-container` div
- **JavaScript Template**: `desktopFieldContainer` generates field HTML dynamically
- **Solution**: Selenium WebDriver to execute JavaScript and wait for content

### 3. FIELD INFORMATION EXTRACTION
- **Field Container**: `div.field-container.p-0.d-block` with `sport` attribute
- **Field Name**: Text content of `div.s18-500` within field container
- **Slot Button**: `div.field_slot_btn` with attributes:
  - `field-name`: Field name
  - `field-id`: Field ID
  - `total-slots`: Number of available slots
- **Slot Status**: Text content of `span.slot-available-text` within slot button

### 4. TIME SLOT EXTRACTION
- **Time Slot Items**: `div.field-slot-item` elements within field containers
- **Disabled Slots**: Have `is-disabled="true"` attribute (NOT class-based)
- **Available Slots**: Have `is-disabled="false"` attribute
- **Slot Attributes**:
  - `slot-id`: Unique slot identifier
  - `field-id`: Associated field ID
  - `date`: Slot date
  - `start-time`: Start time
  - `end-time`: End time
  - `price`: Slot price
  - `is-disabled`: Boolean string ("true" or "false")

### 5. FIELD SPORT TYPE DETECTION
- **Field Description**: Check `div.field_desc_point` class content
- **Tennis Detection**: Look for "tennis" in field description text
- **Padel Detection**: Look for "padel" in field description text
- **Fallback**: Use `sport` attribute if field_desc_point not found
- **Sport Type**: Determines if field is Tennis, Padel, or Unknown

### 6. CABOR-BASED FIELD FILTERING
- **CABOR 7**: Tennis only - excludes all padel fields
- **CABOR 12**: Padel only - excludes all tennis fields
- **Other CABOR**: Include all field types
- **Filtering Logic**: Applied to both field extraction and time slot processing
- **Exclusion Messages**: Shows "EXCLUDED (CABOR X)" for filtered fields

## IMPLEMENTATION APPROACH

### 1. DUAL SCRAPING STRATEGY
- **Static Scraping**: For venue discovery and basic information
- **Selenium Scraping**: For dynamic content and slot information
- **Fallback Mechanism**: Static scraping if Selenium fails

### 2. CONFIGURATION MANAGEMENT
```python
CONFIG = {
    'base_url': 'https://ayo.co.id',
    'venues_path': '/venues',
    'sortby': 5,
    'tipe': 'venue',
    'lokasi': 'Kota+Jakarta+Selatan',
    'cabor': 7,  # 7=Tennis, 12=Padel
    'max_venues_to_test': 3,  # Limit for testing
    'use_selenium': True
}
```

**Key Parameters:**
- `BASE_URL`: Website base URL (default: https://ayo.co.id)
- `VENUES_PATH`: Venues endpoint path (default: /venues)
- `SORTBY`: Sort parameter (default: 5)
- `TIPE`: Type filter (default: venue)
- `LOKASI`: Location filter (default: empty)
- `CABOR`: Sport category filter (7=Tennis, 12=Padel, default: 7)
- `MAX_VENUES_TO_TEST`: Limit venues for testing (default: 3)
- `USE_SELENIUM`: Enable Selenium scraping (default: True)
- `MAX_PAGES`: Maximum pages to scrape (default: 1)

### 3. SELENIUM INTEGRATION
- **WebDriver Setup**: Chrome with headless mode
- **Wait Strategy**: Wait for `field-list-container` to be populated
- **Element Detection**: CSS selectors for field containers and slot buttons

### 4. CABOR-BASED FILTERING
- **Sport-Specific Results**: Filter fields based on queried sport category
- **Tennis Only (CABOR=7)**: Excludes all padel fields and their time slots
- **Padel Only (CABOR=12)**: Excludes all tennis fields and their time slots
- **Efficient Processing**: Skips time slot extraction for excluded fields
- **Clear Feedback**: Shows exclusion messages for filtered fields

### 5. DRY RUN FEATURE
- **Preview Mode**: Shows what will be scraped without actually scraping
- **Pagination Analysis**: Displays total pages available and pages to scrape
- **Venue Count Detection**: Attempts to get actual venue count from `count_drop` class
- **Selenium Fallback**: Uses Selenium to get JavaScript-populated count_drop values
- **Venue Estimation**: Falls back to estimated total venues based on venues per page
- **Configuration Summary**: Shows all current settings
- **URL Preview**: Displays the exact URL that will be used
- **Usage**: `python venue_scraper.py --dry-run` or `make dry-run`

### 6. DATA EXTRACTION WORKFLOW
1. **Venue Discovery**: Scrape venue list pages for venue names and URLs
2. **Pagination Handling**: Determine total pages and scrape specified number
3. **Venue Detail Scraping**: For each venue, visit detail page
4. **JavaScript Execution**: Use Selenium to render dynamic content
5. **Field Information**: Extract field names, IDs, and availability status
6. **Sport Type Detection**: Determine if field is tennis or padel
7. **CABOR Filtering**: Include only fields matching queried sport category
8. **Time Slot Data**: Extract available time slots with pricing (only for included fields)
9. **Data Aggregation**: Combine venue and slot information
10. **Output Generation**: Save results to text and JSON files

## TECHNICAL IMPLEMENTATION DETAILS

### 1. URL CONSTRUCTION
```python
def build_venues_url(self, page=1):
    params = []
    params.append(f"sortby={self.config['sortby']}")
    params.append(f"tipe={self.config['tipe']}")
    params.append(f"lokasi={self.config['lokasi']}")
    params.append(f"cabor={self.config['cabor']}")
    
    if page > 1:
        params.append(f"page={page}")
    
    query_string = "&".join(params)
    return f"{self.base_url}{self.config['venues_path']}?{query_string}"
```

### 2. SELENIUM WAIT STRATEGY
```python
# Wait for field-list-container to be populated
wait.until(lambda driver: driver.find_element(By.ID, "field-list-container").text.strip() != "")

# Additional wait for field containers to render
time.sleep(3)
```

### 3. FIELD INFORMATION EXTRACTION
```python
# Find field containers
field_containers = self.driver.find_elements(By.CSS_SELECTOR, "div.field-container")

# Extract field information
for container in field_containers:
    field_name_div = container.find_element(By.CSS_SELECTOR, "div.s18-500")
    field_name = field_name_div.text.strip()
    
    slot_button = container.find_element(By.CSS_SELECTOR, "div.field_slot_btn")
    field_id = slot_button.get_attribute('field-id')
    slot_status = slot_button.find_element(By.CSS_SELECTOR, "span.slot-available-text").text.strip()
```

### 4. TIME SLOT EXTRACTION
```python
# Find time slots within each field container
for i, container in enumerate(field_containers):
    slot_items = container.find_elements(By.CSS_SELECTOR, "div.field-slot-item")
    
    for slot in slot_items:
        # Check is-disabled attribute (not class)
        is_disabled_attr = slot.get_attribute('is-disabled')
        is_disabled = is_disabled_attr == 'true' if is_disabled_attr else True
        
        if not is_disabled:  # Available slot
            slot_data = {
                'slot_id': slot.get_attribute('slot-id'),
                'field_id': slot.get_attribute('field-id'),
                'date': slot.get_attribute('date'),
                'start_time': slot.get_attribute('start-time'),
                'end_time': slot.get_attribute('end-time'),
                'price': slot.get_attribute('price'),
                'field_name': container.find_element(By.CSS_SELECTOR, "div.s18-500").text.strip()
            }
```

### 5. FIELD SPORT TYPE DETECTION
```python
# Check field_desc_point to determine sport type
field_sport_type = 'Unknown'
try:
    field_desc_point = container.find_element(By.CSS_SELECTOR, "div.field_desc_point")
    field_desc_content = field_desc_point.text.strip()
    
    # Check if it's tennis or padel based on content
    if 'tennis' in field_desc_content.lower():
        field_sport_type = 'Tennis'
    elif 'padel' in field_desc_content.lower():
        field_sport_type = 'Padel'
    else:
        # Fallback to checking the sport attribute
        field_sport_type = sport
except Exception as e:
    field_sport_type = sport
```

### 6. CABOR-BASED FIELD FILTERING
```python
def should_include_field(self, field_sport_type):
    """Check if field should be included based on CABOR"""
    # CABOR 7 = Tennis, CABOR 12 = Padel
    if self.cabor == 7:  # Tennis only
        return field_sport_type.lower() == 'tennis'
    elif self.cabor == 12:  # Padel only
        return field_sport_type.lower() == 'padel'
    else:  # Other CABOR values - include all fields
        return True

# Apply filtering during field extraction
if self.should_include_field(field_sport_type):
    field_info = {
        'field_name': field_name,
        'field_sport_type': field_sport_type,
        'slot_status': slot_status
    }
    fields.append(field_info)
else:
    print(f"Field {field_name} ({field_sport_type}) - EXCLUDED (CABOR {self.cabor})")
```

### 7. DRY RUN IMPLEMENTATION
```python
def dry_run(self):
    """Perform a dry run to show what would be scraped without actually scraping"""
    print("=" * 60)
    print("DRY RUN - Venue Scraping Preview")
    print("=" * 60)
    
    # Build the URL for the first page
    first_page_url = self.build_venues_url(page=1)
    print(f"📋 Venues List URL: {first_page_url}")
    
    # Fetch first page to get pagination info
    soup = self.get_page_content(first_page_url)
    if not soup:
        print("❌ Failed to fetch first page for dry run")
        return
    
    # Get total pages
    total_pages = self.get_total_pages(soup)
    print(f"📄 Total pages available: {total_pages}")
    
    # Get venues from first page to estimate total
    venues_page_1 = self.extract_venue_info(soup)
    venues_per_page = len(venues_page_1)
    print(f"🏟️  Venues per page: {venues_per_page}")
    
    # Calculate what will be scraped
    max_pages_config = self.config.get('max_pages', 1)
    if max_pages_config == 0:
        pages_to_scrape = total_pages
        print(f"📊 Pages to scrape: ALL ({total_pages} pages)")
    else:
        pages_to_scrape = min(max_pages_config, total_pages)
        print(f"📊 Pages to scrape: {pages_to_scrape} (limited by MAX_PAGES={max_pages_config})")
    
    # Estimate total venues
    estimated_total_venues = pages_to_scrape * venues_per_page
    print(f"🎯 Estimated total venues: {estimated_total_venues}")
    
    return {
        'total_pages': total_pages,
        'pages_to_scrape': pages_to_scrape,
        'venues_per_page': venues_per_page,
        'estimated_total_venues': estimated_total_venues,
        'url': first_page_url
    }
```

### 8. PAGINATION FIX
```python
# Fixed pagination logic to handle MAX_PAGES=0 (scrape all pages)
if max_pages == 0:
    pages_to_scrape = total_pages
else:
    pages_to_scrape = min(max_pages, total_pages)

print(f"Will scrape {pages_to_scrape} pages total")

for page_num in range(2, pages_to_scrape + 1):
    # Scrape each page...
```

### 9. COUNT_DROP IMPLEMENTATION
```python
def get_total_venues_count_with_selenium(self, url):
    """Extract total venue count from count_drop class using Selenium"""
    try:
        self.single_venue_scraper.driver.get(url)
        
        # Wait for count_drop to be populated
        wait = WebDriverWait(self.single_venue_scraper.driver, 15)
        
        try:
            # Wait for count_drop element to have innerHTML content
            wait.until(lambda driver: 
                driver.find_element(By.CSS_SELECTOR, ".count_drop").get_attribute('innerHTML').strip() != ""
            )
            
            # Now get the element
            count_element = self.single_venue_scraper.driver.find_element(By.CSS_SELECTOR, ".count_drop")
            count_text = count_element.get_attribute('innerHTML').strip()
            
            # Extract number from text
            numbers = re.findall(r'\d+', count_text)
            if numbers:
                return int(numbers[0])
                
        except TimeoutException:
            print("⚠️  Timeout waiting for count_drop to be populated")
            return None
            
    except Exception as e:
        print(f"Error extracting total venues count with Selenium: {e}")
        return None
```

**JavaScript Source**: The `count_drop` element is populated by JavaScript:
```javascript
$(document).ready(function () {
    $('.count_drop').html(111);
});
```

**Key Points**:
- The element is populated via `innerHTML`, not `textContent`
- Must wait for JavaScript to execute after page load
- Uses Selenium WebDriverWait to ensure content is loaded
- Successfully extracts actual venue count (e.g., 111 venues)

## DEBUGGING APPROACH

### 1. STATIC HTML ANALYSIS
- Check for JavaScript templates in page source
- Look for empty containers that get populated dynamically
- Identify CSS selectors for target elements

### 2. SELENIUM DEBUGGING
- Wait for specific elements to appear
- Log element counts and content
- Handle timeouts gracefully

### 3. ERROR HANDLING
- Fallback mechanisms for failed requests
- Graceful degradation when Selenium fails
- Comprehensive logging for troubleshooting

## TEST RESULTS

### CABOR-Based Filtering Effectiveness

**CABOR=7 (Tennis Only) Test:**
- ✅ **Wins Arena - Kuningan**: Tennis Court - Mizone (Tennis) - Included
- ❌ **Padel Court A - Ungu (Padel)** - EXCLUDED (CABOR 7)
- ❌ **Padel Court B - Hijau (Padel)** - EXCLUDED (CABOR 7)
- ❌ **Padel Court C - Kuning (Padel)** - EXCLUDED (CABOR 7)
- **Result**: Only tennis fields and their time slots shown

**CABOR=12 (Padel Only) Test:**
- ❌ **Tennis Court - Mizone (Tennis)** - EXCLUDED (CABOR 12)
- ✅ **Padel Court A - Ungu (Padel)** - Included
- ✅ **Court 1 (Padel)** - Included
- ✅ **Court 2 (Padel)** - Included
- **Result**: Only padel fields and their time slots shown

**Performance Benefits:**
- **Efficient Processing**: Skips time slot extraction for excluded fields
- **Clean Output**: No irrelevant fields cluttering results
- **Sport-Specific Results**: Perfect for targeted sport facility searches
- **Clear Feedback**: Shows exclusion messages for transparency

## PROJECT STRUCTURE

```
ayo-scrape/
├── requirements.txt          # Dependencies (requests, beautifulsoup4, lxml, selenium)
├── single_venue_scraper.py  # Enhanced scraper with Selenium support
├── venue_scraper.py         # Main scraper with venue discovery and dry run
├── config.env               # Configuration file
├── Makefile                 # Build automation (run, dry-run, setup)
├── knowledge.txt            # This documentation
├── venues_output.txt        # Output file (generated, gitignored)
├── venues_data.json         # JSON output (generated, gitignored)
└── venv/                   # Virtual environment
```

## MAKEFILE TARGETS

- **`make run`**: Run the full venue scraper
- **`make dry-run`**: Preview what will be scraped without actually scraping
- **`make setup`**: Create virtual environment and install dependencies
- **`make help`**: Show available targets

## KEY LEARNINGS

### 1. JAVASCRIPT RENDERING
- Many modern websites load content dynamically
- Static HTML scraping insufficient for JavaScript-heavy sites
- Selenium WebDriver essential for full content access

### 2. URL ENCODING PITFALLS
- Different websites expect different URL encoding formats
- Manual query string construction sometimes necessary
- Test URL formats with actual website behavior

### 3. WAIT STRATEGIES
- Dynamic content requires waiting for elements to load
- Multiple wait strategies may be needed
- Additional delays sometimes necessary for complete rendering

### 4. ATTRIBUTE vs CLASS DETECTION
- **Critical Learning**: Disabled state stored in attributes, not classes
- **Wrong Approach**: Looking for `field-slot-item-disabled` in class name
- **Correct Approach**: Check `is-disabled="true"` attribute value
- **Impact**: This distinction is crucial for accurate data extraction

### 5. NESTED ELEMENT SEARCHING
- Time slots are nested within field containers
- Must search within each field container individually
- Field name association requires parent container context

### 6. ERROR HANDLING
- Robust error handling crucial for production use
- Fallback mechanisms prevent complete failure
- Comprehensive logging aids debugging

## USAGE INSTRUCTIONS

### 1. SETUP
```bash
pip install -r requirements.txt
```

### 2. CONFIGURATION
Modify `CONFIG` dictionary in `venue_scraper.py`:
- `max_venues_to_test`: Number of venues to process (0 = all)
- `use_selenium`: Enable/disable Selenium
- Search parameters: `sortby`, `cabor`, `lokasi`

### 3. EXECUTION
```bash
python venue_scraper.py
```

### 4. OUTPUT
- `venues_output.txt`: Human-readable results with time slots
- `venues_data.json`: Structured JSON data with complete slot information

### 5. SAMPLE OUTPUT
```
✅ Wins Arena - Kuningan | url -> https://ayo.co.id/v/wins-arena-kuningan | slot available -> 1 available fields
    Field: Tennis Court - Mizone - 1 Jadwal Tersedia
    Available time slots:
      Tennis Court - Mizone: 2025-10-04 23:00-00:00 - Rp275000
```

## FUTURE ENHANCEMENTS

### 1. PERFORMANCE OPTIMIZATION
- Parallel processing for multiple venues
- Caching mechanisms for repeated requests
- Database storage for large datasets

### 2. FEATURE ADDITIONS
- Real-time monitoring of slot availability
- Price tracking and alerts
- Integration with booking systems

### 3. ROBUSTNESS IMPROVEMENTS
- Retry mechanisms for failed requests
- Rate limiting to avoid blocking
- Proxy rotation for large-scale scraping

## CRITICAL SUCCESS FACTORS

### 1. ATTRIBUTE vs CLASS DETECTION
The most critical learning was understanding that disabled state is stored in HTML attributes, not CSS classes:
- **❌ Wrong**: `'field-slot-item-disabled' in classes`
- **✅ Correct**: `slot.get_attribute('is-disabled') == 'true'`

### 2. NESTED ELEMENT STRUCTURE
Time slots must be searched within their parent field containers:
- Search within each `div.field-container` individually
- Associate slots with their field names from parent context
- Handle multiple field containers per venue

### 3. JAVASCRIPT RENDERING TIMING
- Wait for `field-list-container` to be populated
- Additional delay for complete field rendering
- Handle dynamic content loading gracefully

## API SUPPORT IMPLEMENTATION

### OVERVIEW
As an alternative to Selenium scraping, the scraper now supports direct API calls to fetch slot availability data. This approach is faster and more reliable than browser automation.

### API ENDPOINT DISCOVERY
- **API URL**: `https://ayo.co.id/venues-ajax/op-times-and-fields`
- **Parameters**: `venue_id={id}&date={YYYY-MM-DD}`
- **Response Format**: JSON with `op_time` and `fields` array

### API RESPONSE STRUCTURE
```json
{
    "op_time": {
        "day": "Minggu",
        "dow": 0,
        "is_open": true,
        "hours": [{"open": "00:00", "close": "01:00"}, ...]
    },
    "fields": [
        {
            "field_id": 1743,
            "field_name": "Tennis Court - Mizone",
            "sport_id": 7,
            "total_available_slots": 5,
            "special_notes": null,
            "slots": [
                {
                    "id": 34243297,
                    "start_time": "00:00:00",
                    "end_time": "01:00:00",
                    "price": 275000,
                    "date": "2025-10-05",
                    "is_available": 1,
                    ...
                }
            ]
        }
    ]
}
```

### CONFIGURATION OPTIONS

#### New Config Parameters
- **USE_API**: `True/False` - Enable API mode instead of Selenium
- **DATE**: `YYYY-MM-DD` - Target date for slot queries
- **VENUE_ID**: Extracted from HTML `id="venue-{id}"` attribute

#### Configuration Hierarchy
1. **API Mode** (`USE_API=True`): Direct JSON API calls
2. **Selenium Mode** (`USE_SELENIUM=True`): Browser automation
3. **Static Mode** (`USE_SELENIUM=False`): HTML parsing only

### CBOR FILTERING WITH API
- **sport_id**: 7 = Tennis, 12 = Padel
- **Filtering**: Only process fields matching `config['cabor']` value
- **Automatic Detection**: Sport type identification from API data

### IMPLEMENTATION BENEFITS

#### Performance Advantages
- **Speed**: No browser overhead, direct HTTP requests
- **Reliability**: No JavaScript execution failures
- **Scalability**: Can process many venues quickly
- **Resource Usage**: Minimal system requirements

#### Data Quality
- **Structured Data**: Clean JSON response format
- **Complete Information**: All slot details in single request
- **Accuracy**: Server-side data, no parsing errors
- **Consistency**: Uniform data format across all venues

### API vs SELENIUM COMPARISON

| Aspect | API Mode | Selenium Mode |
|--------|----------|---------------|
| **Speed** | ⚡ Fast (HTTP requests) | 🐌 Slow (browser automation) |
| **Reliability** | ✅ High (direct JSON) | ⚠️ Variable (JS rendering) |
| **Resource Usage** | 💚 Low (no browser) | 🔴 High (full browser) |
| **Data Accuracy** | ✅ Perfect (server data) | ⚠️ Depends on rendering |
| **Maintenance** | ✅ Stable (API contracts) | 🔶 Browser compatibility issues |

### USAGE EXAMPLES

#### Enable API Mode
```bash
# config.env
USE_API=True
USE_SELENIUM=False
DATE=2025-01-15
MAX_VENUES_TO_TEST=5
```

#### Switch Between Modes
```bash
# API Mode
source venv/bin/activate && source config.env && python venue_scrape.py

# Selenium Mode  
# Change config: USE_API=False, USE_SELENIUM=True

# Dry Run Preview
python venue_scrape.py --dry-run
```

### TECHNICAL IMPLEMENTATION

#### Venue ID Extraction
- **Source**: HTML venue card `id="venue-{id}"` attribute
- **Processing**: Parse integer from `venue-` prefix
- **Validation**: Ensure venue_id exists before API call

#### Error Handling
- **Missing venue_id**: Skip venue with warning
- **API Failures**: Graceful fallback with error messages
- **Invalid JSON**: Handle malformed responses

#### Filtering Logic
- **Sport Category**: Only include fields with matching `sport_id`
- **Available Slots**: Filter slots with `is_available: 1`
- **Date Matching**: Query specific date as configured

### ENHANCED OUTPUT FORMAT

#### Dynamic Header
The output includes a dynamic title that shows the specific date and location being queried: "VENUE SCRAPING RESULTS FOR DATE YYYY-MM-DD IN LOCATION". This clearly identifies the scope of the search results.

#### Filtered Results
The scraper now automatically filters out venues without available slots, showing only venues that have actual booking opportunities. This eliminates visual clutter and focuses on actionable information.

#### Hierarchical Structure
The output format organizes slot information hierarchically:

```
VENUE SCRAPING RESULTS FOR DATE 2025-10-11 IN KOTA JAKARTA SELATAN
==================================================================

Total venues found: 20
Venues with available slots: 14

VENUES WITH AVAILABLE SLOTS:
==========================================
1. Venue Name
   URL: venue_url
   • Field Name (Sport Type):
     date start_time-end_time (formatted_price)
     date start_time-end_time (formatted_price)
     ...
2. Another Venue Name
   URL: another_venue_url
   • Field Name (Sport Type):
     ...
```

#### Output Features
- **Dynamic Title**: Shows date and location being queried for context
- **Filtered Results**: Only venues with available slots are shown (eliminates empty venues)
- **Field-Level Organization**: Time slots grouped by field for clarity
- **Time Format**: `HH:MM:SS-HH:MM:SS` format for precise timing
- **Price Formatting**: Indonesian Rupiah with comma separators (Rp275,000)
- **Date Display**: YYYY-MM-DD format for easy identification
- **Slot Limiting**: Shows up to 12 slots per field to prevent overwhelming output
- **Clear Statistics**: Shows total venues found vs venues with available slots
- **Backwards Compatibility**: Supports both API and Selenium data structures

#### Example Output Format
```
VENUE SCRAPING RESULTS FOR DATE 2025-10-11 IN KOTA JAKARTA SELATAN
==================================================================

Total venues found: 20
Venues with available slots: 14

VENUES WITH AVAILABLE SLOTS:
==========================================
1. Wins Arena - Kuningan
   URL: https://ayo.co.id/v/wins-arena-kuningan
   • Tennis Court - Mizone (Tennis):
     2025-10-11 00:00:00-01:00:00 (Rp275,000)
     2025-10-11 01:00:00-02:00:00 (Rp275,000)
     2025-10-11 05:00:00-06:00:00 (Rp235,000)
   • Tennis Court B - Grey (Tennis):
     2025-10-11 06:00:00-07:00:00 (Rp250,000)
     2025-10-11 07:00:00-08:00:00 (Rp250,000)

2. Exploria Court  
   URL: https://ayo.co.id/v/exploria-court
   • Touch Tennis Court (Tennis):
     2025-10-11 06:00:00-07:00:00 (Rp120,000)
     2025-10-11 07:00:00-08:00:00 (Rp120,000)
```

#### File Outputs
- **venues_output.txt**: Human-readable hierarchical format
- **venues_data.json**: Structured JSON for programmatic access
- **Console Output**: Real-time processing with progress indicators

## CONCLUSION

This project demonstrates a complete web scraping solution that handles both static and dynamic content. The key success factors were:

1. **Understanding the target website's architecture**
2. **Identifying JavaScript rendering requirements**
3. **Implementing appropriate wait strategies**
4. **Building robust error handling**
5. **Creating a flexible, configurable system**
6. **Correctly identifying attribute-based state detection**
7. **Understanding nested element relationships**

The solution successfully extracts venue information, field availability, and detailed time slot data (including field names, dates, times, and prices) from a JavaScript-heavy website, providing a solid foundation for further development and enhancement.

## FINAL WORKING FEATURES

✅ **Venue Discovery**: Scrapes venue list pages with pagination
✅ **Field Information**: Extracts field names, IDs, and availability status  
✅ **Detailed Time Slot Extraction**: Finds available slots with `is-disabled="false"`
✅ **Complete Slot Data**: Field name, date, start time, end time, and price
✅ **Enhanced Output Format**: Hierarchical display with slots organized by field
✅ **Dynamic Title**: Shows date and location being queried for context
✅ **Filtered Output**: Only shows venues with available slots (no empty venues)
✅ **Selenium Integration**: Handles JavaScript-rendered content
✅ **API Support**: Direct JSON API calls for faster slot data retrieval
✅ **Data Source Toggle**: Choose between API (fast) or Selenium (reliable)
✅ **CABOR Filtering**: Automatic sport type filtering (Tennis/Padel)
✅ **Date Configuration**: Configurable target dates for API queries
✅ **Venue ID Extraction**: Automatic venue ID parsing from HTML
✅ **Error Handling**: Robust fallback mechanisms
✅ **Dry Run Mode**: Preview what will be scraped before execution
✅ **Configuration Management**: Environment-based config with defaults
✅ **Multiple Output Formats**: Text and JSON output files
